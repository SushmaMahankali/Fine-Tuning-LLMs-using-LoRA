{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONO9W+7jAD3yMxQy1/qpsu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SushmaMahankali/Fine-Tuning-LLMs-using-LoRA/blob/main/Fine_Tuning_LLMs_to_Write_Positive_Reviews_using_LoRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Setting Up the Environment"
      ],
      "metadata": {
        "id": "FnrER9kqrVST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers datasets PEFT trl accelerate"
      ],
      "metadata": {
        "id": "xR0rmb4Uz9z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm using:\n",
        "\n",
        "transformers & datasets: For loading our base model and the IMDB data.\n",
        "\n",
        "peft: The library that contains the functions to use LoRA.\n",
        "\n",
        "accelerate: A helper for running this efficiently on our GPU."
      ],
      "metadata": {
        "id": "Zk0l-QSjrdAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: The “Before” Snapshot – How Does a Base Model Behave?"
      ],
      "metadata": {
        "id": "I-bC5cP6rzZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import load_dataset\n",
        "\n",
        "# The model we want to fine-tune\n",
        "model_name = \"gpt2\"\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Set the padding token if it's not already set\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token # Use the end-of-sequence token as the padding token\n",
        "\n",
        "# Load the model\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# A prompt to test the model\n",
        "prompt = \"The movie started with a captivating scene that\"\n",
        "\n",
        "# Tokenize the input\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "\n",
        "# Generate a completion\n",
        "# We're moving the model and inputs to the GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "inputs = inputs.to(device)\n",
        "\n",
        "# Generate text\n",
        "generate_ids = model.generate(inputs.input_ids, max_length=50)\n",
        "response = tokenizer.decode(generate_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"--- Base Model Response ---\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZfmFmMvz_LH",
        "outputId": "46c8c793-91f5-4b68-9eb9-48e9f86f662e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Base Model Response ---\n",
            "The movie started with a captivating scene that was shot in the middle of the night. The scene was shot in the middle of the night, and the camera was on the ground. The scene was shot in the middle of the night, and the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Data Preparation"
      ],
      "metadata": {
        "id": "MQlH0ed8r4RM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the IMDB dataset\n",
        "dataset = load_dataset(\"imdb\", split=\"train\")\n",
        "\n",
        "# Filter for only positive reviews (label 1)\n",
        "positive_reviews = dataset.filter(lambda example: example[\"label\"] == 1)\n",
        "\n",
        "# To make this demo run quickly, let's just use a small subset of the data\n",
        "small_dataset = positive_reviews.select(range(500)) # Using 500 examples for speed\n",
        "\n",
        "# We need to format our examples into a single text string for the SFTTrainer\n",
        "def format_review(example):\n",
        "    # For this simple task, the text itself is our training data\n",
        "    return {\"text\": \"Review: \" + example[\"text\"] + \" TL;DR: Positive.\"}\n",
        "\n",
        "formatted_dataset = small_dataset.map(format_review)"
      ],
      "metadata": {
        "id": "5TgDmST20RPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Installing LoRA\n",
        "\n",
        "we will define our LoraConfig that tells the peft library how and where to inject its tiny adapter layers:"
      ],
      "metadata": {
        "id": "usIahCgIr7z9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Create the LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=8,  # The rank of the update matrices. A small number is usually sufficient.\n",
        "    lora_alpha=16, # A scaling factor. A good rule of thumb is to set this to 2*r.\n",
        "    target_modules=[\"c_attn\"], # The specific layers to adapt. For GPT-2, this is the attention layer.\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Wrap the base model with the PEFT model\n",
        "peft_model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Let's see how many parameters we are actually training!\n",
        "peft_model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_ICiYpo0SnD",
        "outputId": "e19f14cb-9106-4e7f-d4d1-277e14fbe5d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 294,912 || all params: 124,734,720 || trainable%: 0.2364\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/peft/tuners/lora/layer.py:2174: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: The Training Session\n",
        "\n"
      ],
      "metadata": {
        "id": "Jhnqj-TqsD64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure to run the cells above to define `peft_model` and `tokenizer`\n",
        "\n",
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "# Safety for training\n",
        "peft_model.config.use_cache = False\n",
        "tokenizer.padding_side = \"right\"\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    peft_model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# Tokenize dataset\n",
        "def tokenize_fn(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512,\n",
        "    )\n",
        "\n",
        "tokenized_ds = formatted_dataset.map(\n",
        "    tokenize_fn,\n",
        "    batched=True,\n",
        "    remove_columns=formatted_dataset.column_names,\n",
        ")\n",
        "\n",
        "# Causal LM collator (no MLM)\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-imdb-finetune\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=2,\n",
        "    learning_rate=2e-4,\n",
        "    num_train_epochs=2,\n",
        "    logging_steps=50,\n",
        "    fp16=True,\n",
        "    bf16=False,\n",
        "    remove_unused_columns=False,\n",
        "    # You can disable WANDB logging if not needed by setting report_to=\"none\"\n",
        "    # report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_ds,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "print(\"Training complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "15-diZg40W8-",
        "outputId": "103b5dc9-2abb-47e2-aacd-0a61019c3fae"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='239' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [239/250 1:47:22 < 04:59, 0.04 it/s, Epoch 1.90/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>3.822800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>3.740200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>3.744100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>3.669700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 1:52:48, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>3.822800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>3.740200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>3.744100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>3.669700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>3.717000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: The “After” Snapshot – Our Specialized Model"
      ],
      "metadata": {
        "id": "Iry7S537sMNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's test the fine-tuned model with the same prompt\n",
        "print(\"\\n--- Fine-Tuned Model Response ---\")\n",
        "\n",
        "# The trainer wraps the model, so we use trainer.model\n",
        "fine_tuned_model = trainer.model\n",
        "\n",
        "# Generate text using the fine-tuned model\n",
        "generate_ids = fine_tuned_model.generate(inputs.input_ids, max_length=50)\n",
        "response = tokenizer.decode(generate_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "GpYAHW970_T_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1459e21c-77cf-427b-9720-b7adfb0c932f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Fine-Tuned Model Response ---\n",
            "The movie started with a captivating scene that was shot in a dark room. The scene was shot in a dark room with a white background. The scene was shot in a dark room with a white background. The scene was shot in a dark room\n"
          ]
        }
      ]
    }
  ]
}